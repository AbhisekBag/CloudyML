{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i3tLT86ewqj"
      },
      "source": [
        "#                       <h1><center>**REGULARIZATION**</center></h1>   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNdcLpCyfBJs"
      },
      "source": [
        "# **ABOUT THE FASHION MNIST DATASET**\n",
        "\n",
        "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
        "\n",
        "# Labels\n",
        "\n",
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "- 0  T-shirt/top\n",
        "- 1  Trouser\n",
        "- 2  Pullover\n",
        "- 3  Dress\n",
        "- 4  Coat\n",
        "- 5  Sandal\n",
        "- 6  Shirt\n",
        "- 7  Sneaker\n",
        "- 8  Bag\n",
        "- 9  Ankle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN2PotTQhavz"
      },
      "source": [
        "![](https://www.researchgate.net/profile/Saeed-Reza-Kheradpisheh/publication/342801790/figure/fig2/AS:911232181735425@1594266090934/Sample-images-from-Fashion-MNIST-dataset.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u99vNeeChuBU"
      },
      "source": [
        "# **TOPICS IN THIS ASSIGNMENT**\n",
        "- Importing and understanding dataset\n",
        "- EDA\n",
        "- Preparing data\n",
        "- Building the model\n",
        "- Compiling and fitting the model\n",
        "- Prediction on test data\n",
        "- Evaluating the model\n",
        "- Regularization techniques\n",
        "- Rebuilding the model using dropout\n",
        "- Rebuilding the model using l2\n",
        "- Rebuilding the model using l1\n",
        "- Rebuilding the model using earlystopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqGH1oZgjcyS"
      },
      "source": [
        "### **How To Load Dataset?**\n",
        "- Documentation Link - https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
        "- Video link below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgE78z2JjySw"
      },
      "source": [
        "# **1. Importing & Understanding Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuYImh3vfE_o",
        "outputId": "d03616c4-6356-4898-eda1-26f965b3becc"
      },
      "outputs": [],
      "source": [
        "# Import fashion mnist from tensorflow.keras.datasets\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# load the data using fashion_mnist.load_data and define train_images, train_labels, test_images, test_labels\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf_jf_0hgQKJ",
        "outputId": "c6fced56-9a7c-4a03-8829-9dce0fb83896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the shape of train_images dataset\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IriS32dDgQM4",
        "outputId": "2e895da3-73b7-4c5d-84c2-092d03d2cca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the shape of train_images single image (train_images[0])\n",
        "train_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUO3ko8IgQO-",
        "outputId": "377b05e0-d902-4373-f972-c4f22b78c9ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's look at the first image which will show in the array form \n",
        "train_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sgYnADoVgcr"
      },
      "source": [
        "**Observation from the array output**\n",
        "- You can see value in the array ranging from 0-255 depicting RGB color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEtWD2fngQRV",
        "outputId": "62203e3a-af9f-4d74-a8ce-865b13ca762d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check length of train_labels\n",
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_NLs5c6gQT3",
        "outputId": "6f7ef0bf-4b57-4d38-9dcf-0c2b7a89e2cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the labels of train data\n",
        "train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zns2Dy2jgQWA",
        "outputId": "b472e32f-6aae-4eac-c4d7-104414762762"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check shape of test data\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgPBMAi4gQYW",
        "outputId": "047c7a9d-4e7c-42ac-e8ec-00950207465c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the labels of test data\n",
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd4b9VoUkZuY"
      },
      "source": [
        "## **How to plot an image?**\n",
        "- You can find a simple way to plot an image of MNIST dataset by watching the video below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgnSJt2Ckn_i"
      },
      "source": [
        "# 2. **EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqfZw79Gkpbp"
      },
      "source": [
        "## **How to plot multiple images of the fashion MNIST?**\n",
        "- Follow the comments below in the code to plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "opKFpR7zgQa0",
        "outputId": "443dcddb-c5ed-4a6c-ec72-20b60d82803f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# importing pyplot and numpy for plotting images of 0-9\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# defining subplots (2,5)\n",
        "fig, ax = plt.subplots(2,5) # code here\n",
        "\n",
        "# looping over ax.flatten(), and plotting each digit\n",
        "for i, ax in enumerate(ax.flatten()):\n",
        "    # choosing each digit occuring at its first instance using np.argwhere\n",
        "    im_idx = np.argwhere(train_labels==i)[0][0]# code here\n",
        "    # reshaping the selected digit to (28, 28) from (1, 28, 28)\n",
        "    plottable_image = np.reshape(train_images[im_idx], (28,28))# code here \n",
        "    # now pass this plottable_image to ax.imshow\n",
        "    ax.imshow(plottable_image, cmap='gray_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VPe7BC0zwtM"
      },
      "source": [
        "# **3. Preparing the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii9es-xLz3rx"
      },
      "source": [
        "## **How to prepare the image for model building?**\n",
        "- Follow the comments below to understand the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6CaqwjzFgQdK"
      },
      "outputs": [],
      "source": [
        "# reshape train_images from (60000, 28, 28) to (60000, 28*28)\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "\n",
        "# convert dtype of train_images from uint8 to float32 \n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "# reshape train_images from (10000, 28, 28) to (10000, 28*28)\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "\n",
        "# convert dtype of test_images from uint8 to float32 \n",
        "test_images = test_images.astype('float32') / 255\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebkVBsCfz-mq"
      },
      "source": [
        "# **4.Model 1: Building the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHUJijh_05bi"
      },
      "source": [
        "![](https://curiousily.com/static/2da120014faf76c47fa4294c7206e291/9d0b7/deep-neural-net.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_QgMl-61SzL"
      },
      "source": [
        "### **How To Build Model?**\n",
        "- Documentation Link - https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "- Watch the video below & follow the steps in code cells later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IAzJM1UWclA"
      },
      "source": [
        "**Let's import necessary libraries and define our model by following the comments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6tYgv5m9gQf-"
      },
      "outputs": [],
      "source": [
        "# importing keras and layers from tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# define the model and its network architecture\n",
        "# define three dense layers having first two layers with 512 neurons & activation='relu'\n",
        "# define third layer with 10 neurons & activation = 'softmax'\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qubsu44H2I6P"
      },
      "source": [
        "# **5. Compiling & fitting the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VatVEoygW0iL"
      },
      "source": [
        "**In the next cell, we will compile our model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tefqC6YmhiwI"
      },
      "outputs": [],
      "source": [
        "# compile the model with optimizer='rmsprop', loss='sparse_categorical_crossentropy', & metrics=['accuracy']\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUwuT-VuXD2E"
      },
      "source": [
        "**Then we will fit our model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mAgrClIhiyW",
        "outputId": "7579e68e-8096-4030-e9bf-0287e3a391c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 6ms/step - loss: 0.2298 - accuracy: 0.9298\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0814 - accuracy: 0.9747\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0536 - accuracy: 0.9832\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0368 - accuracy: 0.9880\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0273 - accuracy: 0.9915\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0199 - accuracy: 0.9935\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0153 - accuracy: 0.9952\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.9965\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.0097 - accuracy: 0.9968\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0091 - accuracy: 0.9968\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d526eafa0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model with epochs=10, batch_size=128\n",
        "model.fit(train_images,train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9iLFk3F2VbK"
      },
      "source": [
        "# **6. Prediction on test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g4dnGVcf_mL"
      },
      "source": [
        "**In the next cell, we will take first 10 images of test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zs9hFsMXhi0Q"
      },
      "outputs": [],
      "source": [
        "# define a variable test_digits and store the first 10 images of test data\n",
        "test_digits = test_images[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3AucCDggE1x"
      },
      "source": [
        "**Then we will predict on those 10 images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WBmBsJBphi2y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 208ms/step\n"
          ]
        }
      ],
      "source": [
        "# predict the test_digits using our model\n",
        "predictions = model.predict(test_digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968vzYbNgLkv"
      },
      "source": [
        "**We will check prediction on first image in next cell**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNLrkDOehi5m",
        "outputId": "25036602-1435-4747-b9fe-c70c832dc8fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([7.2958742e-14, 5.0590743e-11, 3.3063247e-11, 9.0601765e-10,\n",
              "       2.7753126e-15, 1.3115564e-13, 1.2100767e-18, 1.0000000e+00,\n",
              "       4.1150815e-13, 8.5560575e-10], dtype=float32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the first image prediction from predictions\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu3EzXUVgT2P"
      },
      "source": [
        "**Previous output shows probability of first image being either one of 10 products. For example, probability of first image being 0 is 8.8619967e-10 which is very very low.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKs6Xp7yhjDi",
        "outputId": "76e17f2c-5dda-4172-cbfd-0392fc5932a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the index having maximum prediction \n",
        "predictions[0].argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_4Guyo0gtKF"
      },
      "source": [
        "**In next cell, we can see the maximum prediction for first image is 0.99 at index 9..**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew8PbTlGhjFy",
        "outputId": "e9a7b264-8048-4d36-daaa-e32ba8368357"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the index value having maximum prediction\n",
        "predictions[0][7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbAPfFJGhBff"
      },
      "source": [
        "**In next cell, we are confirming whether our prediction is right or wrong by checking label.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmSLltyhjIn",
        "outputId": "4183a281-df03-4621-a23c-fd33f2b0cec5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the label for that index having maximum prediction\n",
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euy6Apcz2dDy"
      },
      "source": [
        "# **7. Evaluating the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo32eyxFhLEA"
      },
      "source": [
        "**Now we evaluate our model on unseen data, which is test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSA1Ot3ehjVR",
        "outputId": "fad0b5b8-d1b4-419b-be3a-a5294e7759e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9814\n"
          ]
        }
      ],
      "source": [
        "# check the loss and accuracy for test data using model.evaluate\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)# code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlQitf4NhjXr",
        "outputId": "2f79c70e-5dc5-4fb0-e3a9-872875359afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_acc: 0.9814000129699707\n"
          ]
        }
      ],
      "source": [
        "# print test accuracy\n",
        "print(f\"test_acc: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCj6rwahSYU"
      },
      "source": [
        "## **Observation from evaluation**\n",
        "- We can see that our train accuracy is 91.01% and our test accuracy is 86.11% which clearly shows the case of overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBvB1fsM2waE"
      },
      "source": [
        "# **REGULARIZATION TECHNIQUES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjckmsZF3KUY"
      },
      "source": [
        "## **What is Regularization?**: It is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. It is a way to overcome overfitting problem in machine learning/deep learning problem. This in turn improves the modelâ€™s performance on the unseen data as well.\n",
        "\n",
        "## **Overfittng:** It is a situation where your model performed exceptionally well on train data but was not able to predict test data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpEzEgiy3bzF"
      },
      "source": [
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-7.52.01-PM-e1522832332857.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTCMKSUt3qR-"
      },
      "source": [
        "## **How do we deal with reglarization?**\n",
        "\n",
        "There are multiple ways to deal with regularization\n",
        "- **1. Dropouts**\n",
        "- **2. L2 & L1 regularization**\n",
        "- **3. Early stopping**\n",
        "- **4. Data augmentation**\n",
        "- **5. Tweaking the architecture**\n",
        "\n",
        "We have covered 5th tweaking the architecture method for regularization in our previous assignment of \"Training Neural Networks\". Data augmentation (4th) method will be covered in a separate assignment in computer vision. \n",
        "\n",
        "## **Here we will cover first 3 ways mentioned above to show you how you can avoid overfitting.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVmzGX7cHKaq"
      },
      "source": [
        "# **Dropouts**\n",
        "\n",
        "Dropout technique works by randomly reducing the number of interconnecting neurons within a neural network. At every training step, each neuron has a chance of being left out, or rather, dropped out of the collated contribution from connected neurons.\n",
        "This technique minimizes overfitting because each neuron becomes independently sufficient, in the sense that the neurons within the layers learn weight values that are not based on the cooperation of its neighbouring neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai_sqMNyJEEf"
      },
      "source": [
        "![](https://i.stack.imgur.com/Njnu4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caR7yU42JJKT"
      },
      "source": [
        "## **Dropout in implementation**\n",
        "**Check the doxcumentation of dropout - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout**\n",
        "\n",
        "![](https://www.upgrad.com/blog/wp-content/uploads/2020/10/num-5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PCI0Do6yhjab"
      },
      "outputs": [],
      "source": [
        "# define the 2nd model and its network architecture\n",
        "# define three dense layers having first two layers with 512 neurons & activation='relu'\n",
        "# define third layer with 10 neurons & activation = 'softmax'\n",
        "# add 2 dropout with value 0.3 (one each after first two dense layers)\n",
        "from keras.layers.core import Dropout\n",
        "model2 = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "]) # code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4z9YiMu1mllg"
      },
      "outputs": [],
      "source": [
        "# compile the model with optimizer='rmsprop', loss='sparse_categorical_crossentropy', & metrics=['accuracy']\n",
        "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOYZEopTmln7",
        "outputId": "ba90de8c-c04c-4e54-b705-e5039cf60bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2755 - accuracy: 0.9147\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.1181 - accuracy: 0.9642\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0875 - accuracy: 0.9732\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0708 - accuracy: 0.9786\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0614 - accuracy: 0.9813\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0532 - accuracy: 0.9840\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0472 - accuracy: 0.9856\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0403 - accuracy: 0.9873\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0373 - accuracy: 0.9883\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0337 - accuracy: 0.9893\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d4236e670>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model with epochs=10, batch_size=128\n",
        "model2.fit(train_images,train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTOHBSQtmlp9",
        "outputId": "1bc8d938-e080-4263-e7d9-c1c20dffe3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9814\n"
          ]
        }
      ],
      "source": [
        "# check the loss and accuracy for test data using model.evaluate\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)# code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLI1RG7nmlWo"
      },
      "source": [
        "## **Observation from evaluation**\n",
        "- We can see that our train accuracy is 88.32% and our test accuracy is 88.22% which clearly shows how well dropout dealt with overfitting.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaGsTZzFLElz"
      },
      "source": [
        "# **L2 Regularization**\n",
        "\n",
        "L2 regularization forces weights toward zero but it does not make them exactly zero. L2 regularization acts like a force that removes a small percentage of weights at each iteration. Therefore, weights will never be equal to zero. There is an additional parameter to tune the L2 regularization term which is called regularization rate (lambda).\n",
        "\n",
        "**Note: Choosing an optimal value for lambda is important. If lambda is too high, the model becomes too simple and tends to underfit. On the other hand, if lambda is too low, the effect of regulatization becomes negligible and the model is likely to overfit. If lambda is set to zero, then regularization will be completely removed (high risk of overfitting!).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8k2tuasME6R"
      },
      "source": [
        "![](https://www.tutorialexample.com/wp-content/uploads/2019/11/l2-regularization.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywhg8Q1FRE3P"
      },
      "source": [
        "## **L2 Regularization in implementation**\n",
        "\n",
        "**Check l2 regularization documentation - https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2**\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHsa7YeYQ4AwkZRGN4JXU9D5b-0cy-qusgjw&usqp=CAU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SKhNyD3smlsd"
      },
      "outputs": [],
      "source": [
        "# import regularizers from keras\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# define the 3nd model and its network architecture\n",
        "# define three dense layers having first two layers with 512 neurons , activation='relu', & kernel_regularizer=regularizers.l2(0.0008)\n",
        "# define third layer with 10 neurons & activation = 'softmax'\n",
        "model3 = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.0008)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.0008)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vjamXj8RmlvR"
      },
      "outputs": [],
      "source": [
        "# compile the model with optimizer='rmsprop', loss='sparse_categorical_crossentropy', & metrics=['accuracy']\n",
        "\n",
        "model3.compile(optimizer=\"rmsprop\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEl0M5_qnu_J",
        "outputId": "2233dbb5-2b22-47d7-fdec-f920762cbb36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.7500 - accuracy: 0.9104\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3242 - accuracy: 0.9544\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2560 - accuracy: 0.9602\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2333 - accuracy: 0.9633\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2206 - accuracy: 0.9631\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2118 - accuracy: 0.9656\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2074 - accuracy: 0.9649\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2021 - accuracy: 0.9662\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1985 - accuracy: 0.9669\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1930 - accuracy: 0.9679\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d00d8b7f0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model with epochs=10, batch_size=128\n",
        "\n",
        "model3.fit(train_images, train_labels, batch_size=128, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l601zRvKnvBd",
        "outputId": "c1ddc23e-8695-43db-a13c-361c01eca0d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9814\n"
          ]
        }
      ],
      "source": [
        "# check the loss and accuracy for test data using model.evaluate\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels) # code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRaAfOVRnmZK"
      },
      "source": [
        "## **Observation from evaluation**\n",
        "- We can see that our train accuracy is 88.25% and our test accuracy is 87.33% which shows how well l2 regularization worked with overfitting but it's not better than dropout. We can still see some overfitting.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxK0kJLsnbnw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69fbiqThRe3-"
      },
      "source": [
        "# **L1 Regularization**\n",
        "\n",
        "It adds an L1 penalty that is equal to the absolute value of the magnitude of coefficient, or simply restricting the size of coefficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKaUvY6zRjVd"
      },
      "source": [
        "![](https://miro.medium.com/max/550/1*-LydhQEDyg-4yy5hGEj5wA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtUCGL2zTV73"
      },
      "source": [
        "## **L1 regularization in implementation**\n",
        "\n",
        "**Check l1 regularization documentation - https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1**\n",
        "\n",
        "![](https://www.oreilly.com/library/view/hands-on-machine-learning/9781788393485/assets/54e4fb02-51be-487d-a0d9-a4bbf6682839.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouqFGaui2KQa"
      },
      "outputs": [],
      "source": [
        "# define the 4th model and its network architecture\n",
        "# define three dense layers having first two layers with 512 neurons , activation='relu', & kernel_regularizer=regularizers.l1(0.0008)\n",
        "# define third layer with 10 neurons & activation = 'softmax'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ3zUOuJ2Y82"
      },
      "outputs": [],
      "source": [
        "# compile the model with optimizer='rmsprop', loss='sparse_categorical_crossentropy', & metrics=['accuracy']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhyZNrAj2eOg",
        "outputId": "496798eb-83a2-4999-8f5b-b35e23a67897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 2.9857 - accuracy: 0.7411\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 1.0575 - accuracy: 0.8026\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 11s 22ms/step - loss: 0.9609 - accuracy: 0.8181\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.9164 - accuracy: 0.8260\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.8891 - accuracy: 0.8313\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.8743 - accuracy: 0.8342\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.8603 - accuracy: 0.8386\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.8522 - accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.8450 - accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.8392 - accuracy: 0.8434\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f735f646f90>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model with epochs=10, batch_size=128\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUea-myr2lL-",
        "outputId": "c6d0229a-a0ba-4d0c-9741-0b2b29252ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 0.8673 - accuracy: 0.8308\n"
          ]
        }
      ],
      "source": [
        "# check the loss and accuracy for test data using model.evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YXHpKsoNQv"
      },
      "source": [
        "## **Observation from evaluation**\n",
        "- We can see that our train accuracy is 84.34% and our test accuracy is 83.08% which shows how our training accuracy dropped from 88.32% to 84.3% and our test accuracy dropped from 88.22% to 83.08% as compared to our 2nd model. It does reduce overfitting as compared to our first model, but it did not perform well as compared to our 2nd model and 3rd model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfJEEHuPThu_"
      },
      "source": [
        "# **Early Stopping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iaCHHHVGR4"
      },
      "source": [
        "**What is early stopping?** - Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.\n",
        "\n",
        "**Why we use early stopping?** - Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwNsVQYVGUm"
      },
      "source": [
        "![](https://miro.medium.com/max/567/1*2BvEinjHM4SXt2ge0MOi4w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dINBQLz7Vkkv"
      },
      "source": [
        "## **Early stopping in implementation**\n",
        "**early stopping documentation link - https://keras.io/api/callbacks/early_stopping/**\n",
        "\n",
        "![](https://www.markiiisys.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-26-at-10.33.35-AM-1024x341.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fI491YGPnvD4"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "\n",
        "# define callbacks variable and import earlystopping with monitor='loss', patience=2\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)# code here \n",
        "\n",
        "# define the 5th model and its network architecture\n",
        "# define three dense layers having first two layers with 512 neurons & activation='relu'\n",
        "# define third layer with 10 neurons & activation = 'softmax'\n",
        "\n",
        "model4 = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8WB9jScLnvGP"
      },
      "outputs": [],
      "source": [
        "# compile the model with optimizer='rmsprop', loss='sparse_categorical_crossentropy', & metrics=['accuracy']\n",
        "\n",
        "model4.compile(optimizer=\"rmsprop\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsXrlLtVnvId",
        "outputId": "6a264b20-6e19-402f-cc88-5f1f62efca4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.0823 - val_accuracy: 0.9855\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.0872 - val_accuracy: 0.9841\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.0806 - val_accuracy: 0.9858\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0849 - val_accuracy: 0.9851\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.0843 - val_accuracy: 0.9850\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.0795 - val_accuracy: 0.9857\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.0887 - val_accuracy: 0.9854\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 0.0804 - val_accuracy: 0.9862\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0822 - val_accuracy: 0.9856\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0811 - val_accuracy: 0.9872\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d00d323d0>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model with epochs=10, batch_size=128\n",
        "\n",
        "model4.fit(train_images, train_labels, callbacks=[callbacks], batch_size=128, epochs=10, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc5nZ68TqDzd"
      },
      "source": [
        "## **Observation from evaluation**\n",
        "- We can see that our train accuracy is 91.22% and our test accuracy is 88.43% which shows how well early stopping method worked with overfitting but it's not better than dropout. We can still see some overfitting. However our test accuracy is better than dropout test accuracy. In terms of accuracy it is better but in terms of generalization, our dropout model which is 2nd model is best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6hTM8_V6oG"
      },
      "source": [
        "# **Summary**\n",
        "\n",
        "In this assignment, we learnt how we can implement multiple methods to deal with overfitting. We learnt the implementation of dropout, l2 , l1, early stopping methods of regularization.\n",
        "\n",
        "Now you can play and tweak the parameters to get more insights from these methods. \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Regularization_withoutcode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
