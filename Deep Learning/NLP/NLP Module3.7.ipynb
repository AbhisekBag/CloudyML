{"cells":[{"cell_type":"markdown","metadata":{"id":"kdBOq2ZLXflH"},"source":["<center><u><h1>Assignment 8:- GRU AND Bidirectional LSTM RNN</center></u></h1>"]},{"cell_type":"markdown","metadata":{"id":"rxmv6QahpJSL"},"source":["##Gated Recurrent Unit (GRU)<br>\n","A gated recurrent unit (GRU) is part of a specific model of recurrent neural network that intends to use connections through a sequence of nodes to perform machine learning tasks associated with memory and clustering, for instance, in speech recognition. Gated recurrent units help to adjust neural network input weights to solve the vanishing gradient problem that is a common issue with recurrent neural networks.\n","![](https://production-media.paperswithcode.com/methods/780px-Gated_Recurrent_Unit_type_1.svg.png)\n","\n","Refer:https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n"]},{"cell_type":"markdown","metadata":{"id":"BoFSci4ssJ7F"},"source":["###Architecture of Gated Recurrent Unit:<br>\n","\n","Here we have a GRU cell which more or less similar to an LSTM cell or RNN cell.\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-17-14-24-12.png)\n","\n","At each timestamp t, it takes an input Xt and the hidden state Ht-1 from the previous timestamp t-1. Later it outputs a new hidden state Ht which again passed to the next timestamp.\n","###The first gate is the Reset gate and the other one is the update gate.\n","\n","1. Reset Gate (Short term memory)\n","The Reset Gate is responsible for the short-term memory of the network i.e the hidden state (Ht). Here is the equation of the Reset gate.\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-17-14-34-17.png)\n","\n","The value of rt will range from 0 to 1 because of the sigmoid function. Here Ur and Wr are weight matrices for the reset gate.\n","\n","2. We have an Update gate for long-term memory and the equation of the gate is shown below.\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-17-15-13-00.png)\n","\n","The only difference is of weight metrics i.e Uu and Wu.\n"]},{"cell_type":"markdown","metadata":{"id":"Yd8ECh5qtPBa"},"source":["Refer:https://keras.io/api/layers/recurrent_layers/gru/"]},{"cell_type":"markdown","metadata":{"id":"YtStSLWwu22j"},"source":["###Applications of GRU:<br>\n","Gated Recurrent Unit can be used to improve the memory capacity of a recurrent neural network as well as provide the ease of training a model. The hidden unit can also be used for settling the vanishing gradient problem in recurrent neural networks. <br>\n","It can be used in various applications, including speech signal modelling, machine translation, handwriting recognition, among others."]},{"cell_type":"markdown","metadata":{"id":"kDQKZZfnwM-Y"},"source":["#Bidirectional LSTM RNN"]},{"cell_type":"markdown","metadata":{"id":"eNJOatAivj3E"},"source":["To enable straight (past) and reverse traversal of input (future), Bidirectional RNNs, or BRNNs, are used. A BRNN is a combination of two RNNs - one RNN moves forward, beginning from the start of the data sequence, and the other, moves backward, beginning from the end of the data sequence. The network blocks in a BRNN can either be simple RNNs, GRUs, or LSTMs.<br>\n","\n","when we are dealing with long sequences of data and the model is required to learn relationship between future and past word as well. we need to send data in that manner. To solve this problem bidirectional network was introduced.\n","\n","Let’s take an example, assume we are having a sentence like<<br>\n","![](https://miro.medium.com/max/609/0*u1exzGPAtza4D50o.gif)\n","\n","Here we can not predict the next word with normal RNN network but this can be solved in bidirectional RNN network. Also, RNN network can be LSTM or GRU.\n","\n","Refer:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"]},{"cell_type":"markdown","metadata":{"id":"SXnU1BZezZia"},"source":["In bidirectional LSTM we give the input from both the directions from right to left and from left to right . Make a note this is not a backward propagation this is only the input which is given from both the side. So, the question is how the data is combined in output if we are having 2 inputs.\n","\n","![](https://miro.medium.com/max/700/0*ZsTT3zzTNGF-6OsR.jpg)\n","\n","Generally in normal LSTM network we take output directly as shown in first figure but in bidirectional LSTM network output of forward and backward layer at each stage is given to activation layer which is a neural network and output of this activation layer is considered. This output contains the information or relation of past and future word also."]},{"cell_type":"markdown","metadata":{"id":"QTTCBYYL2vt_"},"source":["Applications\n","BRNN is useful for the following applications:\n","1. Handwriting Recognition\n","2. Speech Recognition\n","3. Dependency Parsing\n","4. Natural Language Processing Tasks.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2ROLnqTU4aBL"},"source":["## Implementation of Bidirectional LSTMs on Tensorflow\n","BiDirectional LSTMs use two LSTMs to train on sequential input. The first LSTM is used on the input sequence as it is. The second LSTM is used on a reversed representation of the input sequence. It helps in supplementing additional context and makes our model fast.\n","\n","## Developing a Deep learning model to identify when an article might be fake news.Here we are using kaggle dataset."]},{"cell_type":"markdown","metadata":{"id":"6b1GImOH5Cpl"},"source":["we will import the necessary libraries.<br>\n","1. **Pandas** for data analysis,<br>\n","2. **Numpy** for calculating N-dimensional array,<br>\n","3. **Tensorflow** is used for multiple tasks but has a particular focus on the training and inference of deep neural networks and  Keras acts as an interface for the TensorFlow library.<br>\n","4. We’ll need word embeddings i.e **Embedding, Dense,LSTM layers and Bidirectional**.<br>\n","5. We’ll have to pad them with zeroes  by using  **pad_sequences** in order to make them of equal length.<br>\n","6. we will need **One-hot** encodes a text into a list of word indexes of size n.<br>\n","7. As we’ll stack all layers on top of each other with model.add, we need **Sequential** for constructing our model.<br>\n","8. It is time to split our data by using **train_test_split** from sklearn helps us to do that. It will split the data into training and testing data.\n","9. Importing **nltk, re, Stopwords,Poterstemmer** for nlp preprocessing.<br>\n","10. finally to check the **accuary** and **confusion matrix** of our model.\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3878,"status":"ok","timestamp":1641477234043,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"0QfX8KZA0rQ4","outputId":"5e497e9a-7228-4fa6-deb8-472510ae50a2"},"outputs":[],"source":["# Import above mentioned Libraries\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n","from tensorflow.keras.utils import pad_sequences\n","from tensorflow import one_hot\n","from tensorflow.keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.metrics import accuracy_score, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"WOxKdBMDDYbH"},"source":["we will define our dataset and then we will see our dataset for overview.\n"]},{"cell_type":"markdown","metadata":{"id":"cSgnGY585t66"},"source":["You can download the dataset from here:<br>\n","https://drive.google.com/file/d/1OazFCF5yHlQPPXiwFA2DuUQ7OnKlUXQp/view?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ze1moevj5Isi"},"outputs":[],"source":["#loading dataset using pandas\n","data = pd.read_csv('/content/drive/MyDrive/NLP_CLoudyML/NLP_Cloudy/Assignment 8/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":821,"status":"ok","timestamp":1641477249004,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"XJMrFfXj9BCx","outputId":"20b539cd-57bb-4905-dd45-00da734ecf38"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b65d980c-a6e1-41fa-99d8-c02b2968cf16\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>author</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n","      <td>Darrell Lucus</td>\n","      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n","      <td>Daniel J. Flynn</td>\n","      <td>Ever get the feeling your life circles the rou...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Why the Truth Might Get You Fired</td>\n","      <td>Consortiumnews.com</td>\n","      <td>Why the Truth Might Get You Fired October 29, ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n","      <td>Jessica Purkiss</td>\n","      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Iranian woman jailed for fictional unpublished...</td>\n","      <td>Howard Portnoy</td>\n","      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b65d980c-a6e1-41fa-99d8-c02b2968cf16')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b65d980c-a6e1-41fa-99d8-c02b2968cf16 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b65d980c-a6e1-41fa-99d8-c02b2968cf16');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   id  ... label\n","0   0  ...     1\n","1   1  ...     0\n","2   2  ...     1\n","3   3  ...     1\n","4   4  ...     1\n","\n","[5 rows x 5 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#partial view of dataset from top\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":548,"status":"ok","timestamp":1641477253539,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"Xo91mGbGDppq","outputId":"068d2158-f897-43b5-e624-c6c06a95a96a"},"outputs":[{"data":{"text/plain":["(20800, 5)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#dimension of dataset i.e shape of dataset"]},{"cell_type":"markdown","metadata":{"id":"hz8Fo9WWDxdw"},"source":["In this dataset there are 20800 rows and 5 columns. There are some categorical and some numerical columns present."]},{"cell_type":"markdown","metadata":{"id":"DfwHGiBCD9DU"},"source":["Now its time to preprocess the data,\n","firstly we will observe the dataset, this means we have to see the data types of the columns. we will check the dataset information using the info() method."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1641477258436,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"dHiE97-vD71K","outputId":"23052aa5-e2ca-4a16-b914-d9cfb8f5998a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20800 entries, 0 to 20799\n","Data columns (total 5 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      20800 non-null  int64 \n"," 1   title   20242 non-null  object\n"," 2   author  18843 non-null  object\n"," 3   text    20761 non-null  object\n"," 4   label   20800 non-null  int64 \n","dtypes: int64(2), object(3)\n","memory usage: 812.6+ KB\n"]}],"source":["#basic dataset information\n"]},{"cell_type":"markdown","metadata":{"id":"3SIme-AjEC-c"},"source":["You can see that the datatypes of each column, number of rows present with non-null values, there are many int float, and remaining are string datatype columns."]},{"cell_type":"markdown","metadata":{"id":"EfwjLDbnEH35"},"source":["Now we will summarize the statistical part by using describe method."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":824,"status":"ok","timestamp":1641477265613,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"pQJXgP21EJGK","outputId":"12da45b6-a9b8-434b-f950-f3eb660b1c09"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-4694a631-b0d9-45c7-abe0-723aabc83282\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id</th>\n","      <td>20800.0</td>\n","      <td>10399.500000</td>\n","      <td>6004.587135</td>\n","      <td>0.0</td>\n","      <td>5199.75</td>\n","      <td>10399.5</td>\n","      <td>15599.25</td>\n","      <td>20799.0</td>\n","    </tr>\n","    <tr>\n","      <th>label</th>\n","      <td>20800.0</td>\n","      <td>0.500625</td>\n","      <td>0.500012</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4694a631-b0d9-45c7-abe0-723aabc83282')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4694a631-b0d9-45c7-abe0-723aabc83282 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4694a631-b0d9-45c7-abe0-723aabc83282');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         count          mean          std  ...      50%       75%      max\n","id     20800.0  10399.500000  6004.587135  ...  10399.5  15599.25  20799.0\n","label  20800.0      0.500625     0.500012  ...      1.0      1.00      1.0\n","\n","[2 rows x 8 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#basic statistics symmary\n"]},{"cell_type":"markdown","metadata":{"id":"wH9MNuHWEPk0"},"source":["Now we have to check for null values, for this, we use the pandas IsNull() method which will give True if the null value is present and False when there are no null values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1641477270343,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"WOrTqkCX8wdd","outputId":"8fc3a619-5315-43cf-b0db-6029a60f0978"},"outputs":[{"data":{"text/plain":["id           0\n","title      558\n","author    1957\n","text        39\n","label        0\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#checking for missing values\n"]},{"cell_type":"markdown","metadata":{"id":"rSNbyc54EjA3"},"source":["We will drop the null values using the dropna method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GFtIk2q80rS"},"outputs":[],"source":["#dropping Null Values using dropna function\n"]},{"cell_type":"markdown","metadata":{"id":"tCDdgJ1-Eq4i"},"source":["We have deleted all null values so that it can not affect the accuracy of the model . Now we will define X and Y as an independent and dependent variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZF81NHvH85cW"},"outputs":[],"source":["#Create a variable for independent feature and take axis=1\n","\n","#Create a variable for dependent feature \n"]},{"cell_type":"markdown","metadata":{"id":"wiZ-NpD4ExIh"},"source":["The key part of NLP is text preprocessing which we perform on independent variable using NLTK library. We will use re library to remove punctuations then we will pass the data from stop words list and then do stemming on the data.\n","First,we will create a copy of independent feature to use them in preprocessing and then we will use  reset_index method to set index of list from ranging 0 to length of that list.After that we will using the re library for removing the punctutions in the dataset. As we have done the preprocessing in previous assignment same we have to do the prepocessing here in this cell."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38884,"status":"ok","timestamp":1641477325890,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"kyhphb6U877a","outputId":"3dd9ef09-d68d-48d8-ff74-c5649c9dac7d"},"outputs":[{"data":{"text/plain":["['hous dem aid even see comey letter jason chaffetz tweet',\n"," 'flynn hillari clinton big woman campu breitbart',\n"," 'truth might get fire',\n"," 'civilian kill singl us airstrik identifi',\n"," 'iranian woman jail fiction unpublish stori woman stone death adulteri',\n"," 'jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',\n"," 'beno hamon win french socialist parti presidenti nomin new york time',\n"," 'back channel plan ukrain russia courtesi trump associ new york time',\n"," 'obama organ action partner soro link indivis disrupt trump agenda',\n"," 'bbc comedi sketch real housew isi caus outrag']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#create copy of independent variable \n","\n","#reset the index \n","\n","#create the porterstemmer object\n","\n","#create a list to append preproccessed sentence\n","\n","#By using Re Expression for preprocessing the data[DOne in previous assignment]\n","\n","\n","\n","\n","\n","\n","\n","\n","#here we can see that corpus contains the words after preprocessing done.\n"]},{"cell_type":"markdown","metadata":{"id":"NDyICqwdmGNx"},"source":["Now we will one hot encode the data as we have word list and we will get the index w.r.t vocab_size.One-hot encodes a text into a list of word indexes of size n.<br>\n","Refer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/one_hot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1641477341660,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"W8nc4RE59ZgF","outputId":"ce113996-745c-4038-befd-1c1829042d7e"},"outputs":[{"data":{"text/plain":["[[4312, 4027, 4022, 3985, 2645, 1451, 2683, 1493, 3691, 3026],\n"," [4786, 2811, 4817, 929, 4421, 3949, 2667],\n"," [1794, 151, 934, 3387],\n"," [763, 414, 2859, 1880, 3557, 4206],\n"," [3300, 4421, 1528, 1175, 3941, 3451, 4421, 2447, 3077, 571],\n"," [3474,\n","  3248,\n","  1572,\n","  4068,\n","  1020,\n","  2070,\n","  2265,\n","  2105,\n","  2554,\n","  570,\n","  1848,\n","  2280,\n","  3702,\n","  3846,\n","  2667],\n"," [4752, 1009, 2634, 4155, 203, 3988, 269, 640, 4869, 3753, 3438],\n"," [4735, 951, 3321, 3668, 3183, 1738, 2070, 3375, 4869, 3753, 3438],\n"," [3377, 3037, 4875, 1681, 2154, 3769, 3661, 523, 2070, 1394],\n"," [3940, 3759, 3928, 4959, 4385, 1251, 2837, 1934]]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["#create vocab size = 5000 \n","\n","#by using one hot function \n","\n","#printing first 10 sentences.\n"]},{"cell_type":"markdown","metadata":{"id":"d4FjtAoknuwn"},"source":["Next step is padding, as the sentences we have are different in size so we have to do padding to make them equal in length. We can use pre or post padding.\n","Here we are pre padding and setting length to 30.\n","Refer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1641477345558,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"eJXvHFRn9mTX","outputId":"15adbb68-3c62-4339-b7ae-eeb37b8eb161"},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0, 4312, 4027,\n","        4022, 3985, 2645, 1451, 2683, 1493, 3691, 3026],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0, 4786, 2811, 4817,  929, 4421, 3949, 2667],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0, 1794,  151,  934, 3387],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,  763,  414, 2859, 1880, 3557, 4206],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0, 3300, 4421,\n","        1528, 1175, 3941, 3451, 4421, 2447, 3077,  571],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0, 3474, 3248, 1572, 4068, 1020, 2070, 2265,\n","        2105, 2554,  570, 1848, 2280, 3702, 3846, 2667],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0, 4752, 1009, 2634,\n","        4155,  203, 3988,  269,  640, 4869, 3753, 3438],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0, 4735,  951, 3321,\n","        3668, 3183, 1738, 2070, 3375, 4869, 3753, 3438],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0, 3377, 3037,\n","        4875, 1681, 2154, 3769, 3661,  523, 2070, 1394],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","        3940, 3759, 3928, 4959, 4385, 1251, 2837, 1934]], dtype=int32)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#create length set to 30\n","\n","#Creating pad sequences and passing parameters are onehot,length and padding=pre\n","\n","#printing first 10 values\n"]},{"cell_type":"markdown","metadata":{"id":"sW6wo1MVod1Q"},"source":["From above all output we can see that how our sentences are preprocessed for the LSTM input. Now we can implement model to train on our data.\n","We will adding all the hidden layers in the cell.\n","We can then define the  model. we can initialize the model variable with Sequential().\n","The first layer is an Embedding layer, which learns a word embedding that in our case has a dimensionality of 40.\n","This is followed by an Bidirectional LSTM layer providing the recurrent segment, and a Dense layer that has one output through Sigmoid a number between 0 and 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_O28fFs9pSh"},"outputs":[],"source":["#setting embedding_vector_features=40 \n","\n","## Define the Keras model and intialize with sequential()\n","\n","#intialize first layer for embedding with vocab_size,embedding_vector_features and length\n","\n","#intialize another layer with LSTM for 100\n","\n","#adding dense layer with 1 output and having activation function of sigmoid\n"]},{"cell_type":"markdown","metadata":{"id":"8ieC7v-0rSa1"},"source":["The model can then be compiled. We do so by specifying the optimizer, the loss function, and the metrics that we had specified before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9_u5EKcrL-4"},"outputs":[],"source":["# Compile the model\n"]},{"cell_type":"markdown","metadata":{"id":"jPHxVxFBrgrT"},"source":["This is also a good place to generate a summary of what the model looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1641477467864,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"D4rzxPRirTfT","outputId":"d8b801ca-650e-4556-9d3a-e32f153fddb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 30, 40)            200000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 200)              112800    \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 313,001\n","Trainable params: 313,001\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["#printing the summary\n"]},{"cell_type":"markdown","metadata":{"id":"bsRWH9_IuBIH"},"source":["Making into pad sequences sentences into numpy array to pass them in training same goes with denpendant feature i.e y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyibQ5nGt2Nq"},"outputs":[],"source":["#converting into numpy array and store them in X for pad sequence sentences\n","\n","#converting into numpy array and store them in Y for dependant Feature i.e y \n"]},{"cell_type":"markdown","metadata":{"id":"RKmovW4crrKs"},"source":["Split the data into training and testing dataset by using train_test_split from sklearn. After spliting, The model is ready for training :).<br>\n","Training the model,we can instruct TensorFlow to start the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31333,"status":"ok","timestamp":1641478593796,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"17p0SRjw9xSU","outputId":"827b3267-81fb-4b82-e824-2baab1167785"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","192/192 [==============================] - 12s 29ms/step - loss: 0.3293 - accuracy: 0.8414 - val_loss: 0.1958 - val_accuracy: 0.9157\n","Epoch 2/5\n","192/192 [==============================] - 5s 24ms/step - loss: 0.1456 - accuracy: 0.9407 - val_loss: 0.1917 - val_accuracy: 0.9186\n","Epoch 3/5\n","192/192 [==============================] - 5s 24ms/step - loss: 0.1018 - accuracy: 0.9623 - val_loss: 0.2108 - val_accuracy: 0.9181\n","Epoch 4/5\n","192/192 [==============================] - 5s 24ms/step - loss: 0.0853 - accuracy: 0.9691 - val_loss: 0.2593 - val_accuracy: 0.9026\n","Epoch 5/5\n","192/192 [==============================] - 5s 24ms/step - loss: 0.0639 - accuracy: 0.9778 - val_loss: 0.2749 - val_accuracy: 0.9132\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f40100b39d0>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["#spliting the data using train_test_split from sklearn \n","\n","# Train the model X train, Y train, batch size=64 , epochs=5 ,and validation data is X test and Y test.\n"]},{"cell_type":"markdown","metadata":{"id":"F9B3xaAbwbXT"},"source":["We can test our model finally on testing data and can check the confusion metrix."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1881,"status":"ok","timestamp":1641478684426,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"Uq1P0EKi9_Xo","outputId":"96b1045f-fcf6-4d1f-e0bc-9a860ec349d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[3118  301]\n"," [ 223 2393]]\n"]}],"source":["#predict the X test \n","\n","#Printing confusion matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":810,"status":"ok","timestamp":1641478850384,"user":{"displayName":"Harsh Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64","userId":"17024393577710317653"},"user_tz":-330},"id":"X7rAD11bv7j6","outputId":"fdd82cf1-7ade-4559-8e1a-57d8cce381af"},"outputs":[{"name":"stdout","output_type":"stream","text":["91.31731565865783\n"]}],"source":["#printing the Accuracy Score\n"]},{"cell_type":"markdown","metadata":{"id":"F9w_62HLw-eZ"},"source":["We got accuracy of 91%. However, we can improve this accuracy by working on different parameters like vocab_size, sentence length, LSTM layer size, number of epochs."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOpSk0MO8r7n+7IBUzmwqXn","collapsed_sections":[],"mount_file_id":"1VXDX29cS6Dm9cctLCDqkEvKLNjoml0qz","name":"NLP_CloudyML_Assignment_8_withoutcode.ipynb","provenance":[{"file_id":"1VXDX29cS6Dm9cctLCDqkEvKLNjoml0qz","timestamp":1641615272058}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
